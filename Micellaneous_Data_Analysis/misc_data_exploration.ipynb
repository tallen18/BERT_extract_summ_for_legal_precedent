{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Counting the total number of Cases in database"
      ],
      "metadata": {
        "id": "vxBrDdWT2hK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qSCK7sVKYqe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHdR7C7gKfJd"
      },
      "outputs": [],
      "source": [
        "summary_file = '/content/gdrive/MyDrive/Thesis/Data/unique_citations_origin_dict.pickle'\n",
        "all_opinion_data = '/content/gdrive/MyDrive/Thesis/Data/case_data.jsonl.xz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AORnF2SQKjKc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import lzma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4LA_Jz1KhVj"
      },
      "source": [
        "Number of cases in the case data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5l9UkP5KzzC"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "\n",
        "with lzma.open(all_opinion_data) as in_file:\n",
        "  for i, line in enumerate(in_file):\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    if count % 100000 == 0:\n",
        "      print(count)\n",
        "\n",
        "print('Final Number:', count)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the Metadata included for a case"
      ],
      "metadata": {
        "id": "ayr64e9X2rnB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pFvqOYVLQ5d"
      },
      "source": [
        "See details of the case data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-RJaDH6LQOq"
      },
      "outputs": [],
      "source": [
        "with lzma.open(all_opinion_data) as in_file:\n",
        "  for i, line in enumerate(in_file):\n",
        "\n",
        "    cases = json.loads(str(line,'utf8'))\n",
        "    print(cases)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hefMjDdiTRHr"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "\n",
        "def draw_tree(node):\n",
        "    G = graphviz.Digraph(format='png')\n",
        "    data_to_graphviz(node, G, parent=None)\n",
        "    G.render('tree', view=True)\n",
        "\n",
        "def data_to_graphviz(data, G, parent):\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            node = str(key)\n",
        "            G.node(node, label=node)\n",
        "            if parent is not None:\n",
        "                G.edge(parent, node)\n",
        "            data_to_graphviz(value, G, node)\n",
        "    elif isinstance(data, list):\n",
        "        for i, item in enumerate(data):\n",
        "            node = str(i)\n",
        "            G.node(node, label=node)\n",
        "            if parent is not None:\n",
        "                G.edge(parent, node)\n",
        "            data_to_graphviz(item, G, node)\n",
        "\n",
        "draw_tree(cases)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing the number of summaries typically in a case"
      ],
      "metadata": {
        "id": "_7swgZnD2x7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWAILVStpMQd"
      },
      "outputs": [],
      "source": [
        "def read_through_pickle(file):\n",
        "\t'''\n",
        "\tThis function just takes in a pickle file and reads it. Currently just returns the dictionary from my pickle file\n",
        "\t'''\n",
        "\tobjects = []\n",
        "\twith (open(file, 'rb')) as openfile:\n",
        "\t\twhile True:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tunique_citations = pickle.load(openfile)\n",
        "\t\t\t\treturn unique_citations #This is just returning the file\n",
        "\t\t\t\tbreak\n",
        "\t\t\texcept EOFError:\n",
        "\t\t\t\treturn \"There was an Error Loading Pickle File!!!\"\n",
        "\n",
        "num_sums = []\n",
        "summaries = read_through_pickle(summary_file)\n",
        "for id in summaries:\n",
        "  num_sum = len(summaries[id])\n",
        "  num_sums.append(num_sum)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvldZI3oqOha"
      },
      "outputs": [],
      "source": [
        "print('Total Number of cases:', len(num_sums))\n",
        "print('Average Summary Length:', sum(num_sums)/len(num_sums))\n",
        "print('Min Num Sums:', min(num_sums))\n",
        "print('Min Num Sums:', max(num_sums))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRGVd90gqutk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "bins = [x for x in range(0, 60, int(60 / 12))]\n",
        "#print(bins)\n",
        "\n",
        "plt.hist(num_sums, bins = bins)\n",
        "plt.xlabel('Opinions')\n",
        "plt.ylabel('Number of Citations')\n",
        "plt.title('Number of Citations per Opinion')\n",
        "\n",
        "# Add a vertical line at 4.17\n",
        "plt.axvline(x=4.17, color='red')\n",
        "\n",
        "# Add a label to the line\n",
        "plt.text(5, plt.ylim()[1]*0.9, 'average = 4.17', ha='left', va='center', color='red')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#print('Average Paragrah Length is', sum(all_tokens) / len(all_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg7ieDq-64-D"
      },
      "source": [
        "### Find where the summaries typcially occur within a case and fit a curve to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwPRqsfa770r"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLxrxLHv75av"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import lzma\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TmB2gnz692K"
      },
      "outputs": [],
      "source": [
        "summary_file = '/content/gdrive/MyDrive/Thesis/Data/unique_citations_origin_dict.pickle'\n",
        "all_opinion_data = '/content/gdrive/MyDrive/Thesis/Data/case_data.jsonl.xz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK1MvTcH7Rk3"
      },
      "outputs": [],
      "source": [
        "def read_through_pickle(file):\n",
        "\t'''\n",
        "\tThis function just takes in a pickle file and reads it. Currently just returns the dictionary from my pickle file\n",
        "\t'''\n",
        "\tobjects = []\n",
        "\twith (open(file, 'rb')) as openfile:\n",
        "\t\twhile True:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tunique_citations = pickle.load(openfile)\n",
        "\t\t\t\treturn unique_citations #This is just returning the file\n",
        "\t\t\t\tbreak\n",
        "\t\t\texcept EOFError:\n",
        "\t\t\t\treturn \"There was an Error Loading Pickle File!!!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Agk2xD7TnQ"
      },
      "outputs": [],
      "source": [
        "def make_summaries_sentences(summaries):\n",
        "  '''\n",
        "  This helper function inputs a list of summaries and returns a list of all the setences included in the summaries\n",
        "  '''\n",
        "  list_of_sentences = []\n",
        "  for summ in summaries:\n",
        "    sentences = sent_tokenize(summ)\n",
        "    list_of_sentences += sentences\n",
        "\n",
        "  return list_of_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEAMCLNC7WWU"
      },
      "outputs": [],
      "source": [
        "id_to_sum = read_through_pickle(summary_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "127pFoMr7ahh"
      },
      "outputs": [],
      "source": [
        "df_dict = {'case_text': [], 'summaries': []} #Will be {case_text: ____, summaries: _____}\n",
        "count = 0\n",
        "with lzma.open(all_opinion_data) as in_file:\n",
        "  for i, line in enumerate(in_file):\n",
        "\n",
        "    if count >= 1500:\n",
        "      break\n",
        "\n",
        "    cases = json.loads(str(line,'utf8'))\n",
        "    case_id = cases['id']\n",
        "\n",
        "    if case_id in id_to_sum:\n",
        "\n",
        "      count += 1\n",
        "\n",
        "      case_text = cases['casebody']['data']['opinions'][0]['text']\n",
        "      case_summaries = [x[1] for x in id_to_sum[case_id]]\n",
        "\n",
        "      df_dict['case_text'].append(case_text)\n",
        "      df_dict['summaries'].append(make_summaries_sentences(case_summaries)) #Making sure the summaries consist of a list of individual sentences\n",
        "\n",
        "print(\"We are are analyzing \", count, \" summaries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm26_1We8YHp"
      },
      "outputs": [],
      "source": [
        "def get_cos_sim(summ, pred):\n",
        "  '''\n",
        "  Calculates the similarity between two sentences\n",
        "  '''\n",
        "  X = summ\n",
        "  Y = pred\n",
        "\n",
        "  X_list = word_tokenize(X) \n",
        "  Y_list = word_tokenize(Y)\n",
        "\n",
        "  sw = stopwords.words('english') \n",
        "  l1 =[];l2 =[]\n",
        "\n",
        "  # remove stop words from the string\n",
        "  X_set = {w for w in X_list if not w in sw} \n",
        "  Y_set = {w for w in Y_list if not w in sw}\n",
        "\n",
        "  # form a set containing keywords of both strings \n",
        "  rvector = X_set.union(Y_set) \n",
        "  for w in rvector:\n",
        "      if w in X_set: l1.append(1) # create a vector\n",
        "      else: l1.append(0)\n",
        "      if w in Y_set: l2.append(1)\n",
        "      else: l2.append(0)\n",
        "  c = 0\n",
        "\n",
        "  for i in range(len(rvector)):\n",
        "        c+= l1[i]*l2[i]\n",
        "\n",
        "  denominator = float((sum(l1)*sum(l2))**0.5)\n",
        "\n",
        "  if denominator == 0:\n",
        "    cosine = 0\n",
        "\n",
        "  cosine = c / denominator\n",
        "\n",
        "  return cosine\n",
        "\n",
        "def is_sent_in_summ(s, relevant_summ):\n",
        "  for summ in relevant_summ:\n",
        "    if get_cos_sim(s, summ) >= 0.7:\n",
        "      #print('1.', s)\n",
        "      #print('2.', summ)\n",
        "      return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qjwi7BVm7hFv"
      },
      "outputs": [],
      "source": [
        "list_of_yes_no = []\n",
        "\n",
        "for i in range(len(df_dict['case_text'])):\n",
        "  holder = []\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print('Completed', i, '/ 1500')\n",
        "\n",
        "  case_text = df_dict['case_text'][i]\n",
        "  summaries = df_dict['summaries'][i]\n",
        "\n",
        "  case_sents = sent_tokenize(case_text)\n",
        "  for s in case_sents:\n",
        "    if is_sent_in_summ(s, summaries):\n",
        "      holder.append(1)\n",
        "    else:\n",
        "      holder.append(0)\n",
        "  \n",
        "  list_of_yes_no.append(holder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2AZ7uk4CDzK"
      },
      "outputs": [],
      "source": [
        "on = '/content/gdrive/MyDrive/Thesis/Data/exploration_holder.pickle'\n",
        "with open(on, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(list_of_yes_no, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-ZVACBICjyU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "list_of_yes_no = pd.read_pickle('/content/gdrive/MyDrive/Thesis/Data/exploration_holder.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA29qDnv942X"
      },
      "outputs": [],
      "source": [
        "#Creating batches of 50 and summing the number of summaries within them for each case\n",
        "import numpy as np\n",
        "\n",
        "num_batches = 50\n",
        "\n",
        "new_list = []\n",
        "for labels in list_of_yes_no:\n",
        "#   batch_size = len(labels) // num_batches\n",
        "#   if len(labels) % num_batches != 0:\n",
        "#     batch_size += 1\n",
        "  batches = np.array_split(labels, num_batches)\n",
        "  #print(batches)\n",
        "  #print(len(batches))\n",
        "\n",
        "  holder = []\n",
        "  for b in batches:\n",
        "    holder.append(sum(b))\n",
        "  \n",
        "  new_list.append(holder)\n",
        "  # print(new_list)\n",
        "  # print(len(new_list))\n",
        "\n",
        "\n",
        "print(len(new_list))\n",
        "print(len(new_list[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU9v0QTd-9HD"
      },
      "outputs": [],
      "source": [
        "sums = [sum(x) for x in zip(*new_list)]\n",
        "print(sums)\n",
        "avg = []\n",
        "for s in sums:\n",
        "  avg.append(s / len(sums))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUfb6a_rysjO"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "bins = np.linspace(0, 1, 50)\n",
        "#print(bins)\n",
        "\n",
        "plt.scatter(bins, avg)\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Number of Golden Summaries')\n",
        "plt.title('The Number of  Golden Summaries as a Function of Position in Judicial Opinion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYwAcpFk4lfh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def gaussian(x, a, b, c):\n",
        "  return a * np.exp(-(x - b)**2 / (2 * c**2))\n",
        "\n",
        "\n",
        "xdata = np.linspace(0, 1, 50)\n",
        "ydata = avg\n",
        "\n",
        "popt, pcov = curve_fit(gaussian, xdata, ydata)\n",
        "\n",
        "plt.scatter(xdata, ydata, label='data')\n",
        "plt.plot(xdata, gaussian(xdata, *popt), 'r-', label='fit')\n",
        "plt.legend()\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Number of Golden Summaries')\n",
        "plt.title('Golden Summaries as a Function of Position')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OkNuvsR3Pb0"
      },
      "outputs": [],
      "source": [
        "p = np.polyfit(xdata, ydata, 2)\n",
        "parabola = np.poly1d(p)\n",
        "\n",
        "plt.scatter(xdata, ydata)\n",
        "plt.plot(xdata, parabola(xdata), c='r')\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Number of Golden Summaries')\n",
        "plt.title('Golden Summaries as a Function of Position')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Analysis of Training and Testing Accuracies for Sentence Classification Model "
      ],
      "metadata": {
        "id": "AnfPVCWP3C0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_accuracy = [79.24908293174136, 81.5572113440767, 81.19195046439629, 81.87350743394191, 82.22430909578749, 82.4095646842428, 82.18759869459943, 82.27832359159308, 82.28307489277466, 82.41335044929397, 82.85999833430499, 82.43364357118234, 84.80903402143423, 84.85531103794837, 84.51152390780874, 85.22455897080349, 85.87335658706735, 85.60392397302269, 85.38793557216549, 85.76047755254322, 85.62630594963159, 86.13607188703466, 85.94986258016158, 85.984143398828, 88.52046320937927, 89.17576387685554, 89.078087375301, 90.17024882520607, 90.59565334048833, 90.0, 89.93578271396989, 90.13182439995026, 89.9153194765204, 90.37227214377407, 89.87257433163988, 90.44467425025853]\n",
        "\n",
        "training_loss = [0.430480321930342, 0.3964337146283573, 0.39501128196866886, 0.3929519767067335, 0.3865943260699299, 0.3877102724666323, 0.38181565659101074, 0.37933855408939743, 0.3795422718904441, 0.37747477374591043, 0.3759234589406678, 0.37488947112882537, 0.3413790395744338, 0.33568213950884784, 0.34381769557964376, 0.3341206460207601, 0.31935306837078714, 0.33081409764073205, 0.3236067652571456, 0.320046638274, 0.3208178548769339, 0.31803280438210846, 0.32008092659040055, 0.3143640331861565, 0.2671699155951551, 0.2565608811336834, 0.2610187681328582, 0.24640176729076232, 0.2331043563802324, 0.2445713055723227, 0.23883051993280932, 0.23023281730260609, 0.23682344964957897, 0.23285277944124774, 0.24410878834286917, 0.22960094136263795]\n",
        "\n",
        "validation_accuracy = [82.59493670886076, 81.33518133518133, 80.77055383556932, 81.73136167590881, 80.71351931330472, 80.06375674350171, 80.08421052631579, 82.392439691619, 79.46350043975374, 81.63696768347931, 80.7128580946036, 81.41714915908464]\n"
      ],
      "metadata": {
        "id": "iDsHnOHoV-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_accuracy))\n",
        "print(len(training_loss))"
      ],
      "metadata": {
        "id": "ep8XP2aEWBAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax1.plot(training_accuracy, label=\"Accuracy\")\n",
        "# ax1.axvline(x=12, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "# ax1.axvline(x=24, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "ax1.set_title(\"Accuracy\")\n",
        "# ax1.text(4, 88, \"Epoch 1\")\n",
        "# ax1.text(16, 88, \"Epoch 2\")\n",
        "# ax1.text(28, 84, \"Epoch 3\")\n",
        "\n",
        "ax2.plot(training_loss, label=\"Loss\")\n",
        "# ax2.axvline(x=12, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "# ax2.axvline(x=24, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "ax2.set_title(\"Loss\")\n",
        "# ax2.text(4, 0.350, \"Epoch 1\")\n",
        "# ax2.text(16, 0.375, \"Epoch 2\")\n",
        "# ax2.text(28, 0.325, \"Epoch 3\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ql0vCyBhWTMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_epoch_a = training_accuracy[25:]\n",
        "final_epoch_l = training_loss[25:]\n",
        "\n",
        "avg_a = sum(final_epoch_a) / len(final_epoch_a)\n",
        "print(avg_a)\n",
        "\n",
        "avg_l = sum(final_epoch_l) / len(final_epoch_l)\n",
        "print(avg_l)"
      ],
      "metadata": {
        "id": "yrtZoCUaW6iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data\n",
        "values = [10, 8, 12, 15, 7]\n",
        "\n",
        "# Create the bar plot\n",
        "plt.bar(range(len(validation_accuracy)), validation_accuracy)\n",
        "\n",
        "# Calculate the average\n",
        "average = sum(validation_accuracy) / len(validation_accuracy)\n",
        "print(average)\n",
        "\n",
        "# Add a horizontal line at the average\n",
        "plt.axhline(y=average, color='red', linestyle='-')\n",
        "plt.text(12.10, 81, \"Average Accuracy:\")\n",
        "plt.text(13.1, 76, \"81.07\")\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title(\"Batch Accuracy of Validation Set\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "swkWMDZAYouy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Number of Examples used in Training and Testing for Sentence Classification Model"
      ],
      "metadata": {
        "id": "O1FJSf4-3O6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ZF9h1Hc7tRZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_location = '/content/gdrive/MyDrive/Thesis/Data/TrainTestBinClass/'\n",
        "file_numbers = ['1000', '2000', '3000', '4000', '5000', '5999', '7000', '8000', '9000', '10000', '11000', '12000']\n",
        "\n",
        "list_of_training_files = []\n",
        "list_of_testing_files = []\n",
        "\n",
        "for number in file_numbers:\n",
        "  train_file = folder_location+number+'_training_bdf.pickle'\n",
        "  test_file = folder_location+number+'_testing_bdf.pickle'\n",
        "  list_of_training_files.append(train_file)\n",
        "  list_of_testing_files.append(test_file)"
      ],
      "metadata": {
        "id": "Kixz1JZ9tXuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "num_training_examples = 0\n",
        "num_testing_examples = 0\n",
        "\n",
        "for i in range(len(list_of_training_files)):\n",
        "\n",
        "  train_df = pd.read_pickle(list_of_training_files[i])\n",
        "\n",
        "  num_training_examples += sum(train_df['labels'])\n",
        "\n",
        "  test_df = pd.read_pickle(list_of_testing_files[i])\n",
        "\n",
        "  num_testing_examples += sum(test_df['labels'])\n",
        "\n",
        "print(num_training_examples)\n",
        "print(num_testing_examples)\n",
        "\n"
      ],
      "metadata": {
        "id": "6d4c07sXtfqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Analysis of Training and Testing Accuracies for Batch Classification Model "
      ],
      "metadata": {
        "id": "HTtpW6s73XD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_training_accuracy = [69.36797926181953, 72.18717139852787, 72.68702463389715, 73.50363000806668, 73.38378120717393, 74.0345937248592, 73.54048319759086, 75.15121589927169, 74.65168243953732, 75.50232716539902, 75.6224791610648, 76.94095215396533, 76.24698310539019, 76.44925056464308, 77.85458585359832, 77.66824395373291, 78.81711885571575, 78.9782199515999, 80.62920812727164, 80.13374899436846, 80.31620012319485]\n",
        "\n",
        "\n",
        "b_training_loss = [0.5586259931489601, 0.5407131295396577, 0.5217077449310854, 0.5157782530749029, 0.5202178691111654, 0.5134308044832389, 0.5155074921376185, 0.49762741674332744, 0.5039870651932404, 0.48448406870220023, 0.4807426348934781, 0.4760561283261429, 0.48007301377298867,  0.4754189599704129, 0.45426987014275017, 0.45878450500266676, 0.4359722288790562, 0.4318719594891626, 0.4199761555632674, 0.4246197180977587, 0.40937716163714744]\n",
        "\n",
        "\n",
        "b_validation_accuracy = [72.79684028634905, 73.39642481598318, 72.93984108967084, 72.91890729189073, 72.59294566253575, 73.43655741001407, 74.65097180399671]"
      ],
      "metadata": {
        "id": "r2nRTMhAn3WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(b_training_accuracy))\n",
        "print(len(b_training_loss))"
      ],
      "metadata": {
        "id": "veogtVDYn7IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax1.plot(b_training_accuracy, label=\"Accuracy\")\n",
        "# ax1.axvline(x=12, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "# ax1.axvline(x=24, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "ax1.set_title(\"Accuracy\")\n",
        "# ax1.text(4, 88, \"Epoch 1\")\n",
        "# ax1.text(16, 88, \"Epoch 2\")\n",
        "# ax1.text(28, 84, \"Epoch 3\")\n",
        "\n",
        "ax2.plot(b_training_loss, label=\"Loss\")\n",
        "# ax2.axvline(x=12, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "# ax2.axvline(x=24, color=(0.7, 0.7, 0.7), linestyle='--')\n",
        "ax2.set_title(\"Loss\")\n",
        "# ax2.text(4, 0.350, \"Epoch 1\")\n",
        "# ax2.text(16, 0.375, \"Epoch 2\")\n",
        "# ax2.text(28, 0.325, \"Epoch 3\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9KFexeX2oDst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Create the bar plot\n",
        "plt.bar(range(len(b_validation_accuracy)), b_validation_accuracy)\n",
        "\n",
        "# Calculate the average\n",
        "average = sum(b_validation_accuracy) / len(b_validation_accuracy)\n",
        "print(average)\n",
        "\n",
        "# Add a horizontal line at the average\n",
        "plt.axhline(y=average, color='red', linestyle='-')\n",
        "plt.text(6.9, 73, \"Average Accuracy:\")\n",
        "plt.text(7.6, 68, \"73.25\")\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title(\"Batch Accuracy of Validation Set\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VeRFECgyoTd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Number of Examples used in Training and Testing for Batch Classification Model"
      ],
      "metadata": {
        "id": "qqsRQOAl3jVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_location = '/content/gdrive/MyDrive/Thesis/Data/BATCHClassTrainTest/'\n",
        "\n",
        "file_numbers = ['2000', '4000', '6000', '8000', '16000', '18000', '19999']\n",
        "\n",
        "list_of_training_files = []\n",
        "list_of_testing_files = []\n",
        "\n",
        "for number in file_numbers:\n",
        "  train_file = folder_location+number+'_batch_training_bdf.pickle'\n",
        "  test_file = folder_location+number+'_batch_testing_bdf.pickle'\n",
        "  list_of_training_files.append(train_file)\n",
        "  list_of_testing_files.append(test_file)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "num_training_examples = 0\n",
        "num_testing_examples = 0\n",
        "\n",
        "for i in range(len(list_of_training_files)):\n",
        "\n",
        "  train_df = pd.read_pickle(list_of_training_files[i])\n",
        "\n",
        "  num_training_examples += sum(train_df['contains_summ'])\n",
        "\n",
        "  test_df = pd.read_pickle(list_of_testing_files[i])\n",
        "\n",
        "  num_testing_examples += sum(test_df['contains_summ'])\n",
        "\n",
        "print(num_training_examples)\n",
        "print(num_testing_examples)"
      ],
      "metadata": {
        "id": "i4CyecSYpniM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}