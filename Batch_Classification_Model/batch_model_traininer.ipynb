{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Steps for Training the Batch Classification Model\n",
        "\n",
        "### 1. Pull the data\n",
        "\n",
        ">* Need to get the trianing and testing data from the Preprocessing data\n",
        "\n",
        "### 2. Build the Dataloader class\n",
        "\n",
        "### 3. Build the Model\n",
        "\n",
        "### 4. Build the Trainer Algorithm\n",
        "\n",
        "### 5. Train the Model\n",
        "\n",
        "### 6. Validate the Model"
      ],
      "metadata": {
        "id": "1UVsUfF91VK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "3OvMskrOvT3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_as_pickle(filename, data):\n",
        "  with open(filename, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(data, handle)"
      ],
      "metadata": {
        "id": "8_IiLqPgfqNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "no3-75Iq04jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        " \n",
        "import pandas as pd \n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "sentenc_model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(sentenc_model_name) "
      ],
      "metadata": {
        "id": "_OExWXGs1fdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "import os\n",
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "id": "-8zK7uyp1isk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05"
      ],
      "metadata": {
        "id": "FvaTnlvd1mo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a manual way of pulling the training / testing files for this model. You can also use the list of files generated at the end of the \"Batch_Model_Preprocessor\" file. "
      ],
      "metadata": {
        "id": "OcI1vjeH063r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_location = '/content/gdrive/MyDrive/Thesis/Data/BATCHClassTrainTest/'\n",
        "\n",
        "file_numbers = ['2000', '4000', '6000', '8000', '16000', '18000', '19999']\n",
        "#file_numbers = ['6000', '8000', '16000', '18000', '19999']"
      ],
      "metadata": {
        "id": "k2l6Heh9X_TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_training_files = []\n",
        "list_of_testing_files = []\n",
        "\n",
        "for number in file_numbers:\n",
        "  train_file = folder_location+number+'_batch_training_bdf.pickle'\n",
        "  test_file = folder_location+number+'_batch_testing_bdf.pickle'\n",
        "  list_of_training_files.append(train_file)\n",
        "  list_of_testing_files.append(test_file)"
      ],
      "metadata": {
        "id": "UEZC0QQhYxKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Builing the Data loader Class"
      ],
      "metadata": {
        "id": "1dI1Z2nw16MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchedCasesData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        document = str(self.data.iloc[index].input_sentences)\n",
        "        document = \" \".join(document.split())\n",
        "\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            [document], \n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'doc_id': torch.tensor(ids[0], dtype=torch.long),\n",
        "            'doc_mask': torch.tensor(mask[0], dtype=torch.long),\n",
        "            'targets': torch.tensor([self.data.iloc[index].contains_summ], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }"
      ],
      "metadata": {
        "id": "7lgXQs-g18zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Model"
      ],
      "metadata": {
        "id": "1Wnf0-eD195F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "class BatchBertClass(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", in_features=768):\n",
        "        super(BatchBertClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(model_name)\n",
        "        self.pre_classifier = torch.nn.Linear(in_features, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 1)\n",
        "        self.classifierSigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, doc_ids, doc_mask):\n",
        "\n",
        "        doc_output = self.l1(input_ids=doc_ids, attention_mask=doc_mask) \n",
        "        doc_embeddings = mean_pooling(doc_output, doc_mask)\n",
        "        \n",
        "        pooler = self.pre_classifier(doc_embeddings) \n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        output = self.classifierSigmoid(output) \n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "zmlB4Z-62yZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "model = BatchBertClass(model_name=sentenc_model_name)\n",
        "model.to(device);\n",
        "\n",
        "loss_function = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "ToHZkVns31Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the training algorithm"
      ],
      "metadata": {
        "id": "l1GDPJvq2Cf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "print_n_steps = 1000\n",
        "EPOCHS = 3 \n",
        "acc_step_holder, loss_step_holder = [], []\n",
        "\n",
        "\n",
        "def train(epoch):    \n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        doc_ids = data['doc_id'].to(device, dtype = torch.long)\n",
        "        doc_mask = data['doc_mask'].to(device, dtype = torch.long) \n",
        "        targets = data['targets'].to(device, dtype = torch.float)  \n",
        "\n",
        "        outputs = model(doc_ids, doc_mask) \n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item() \n",
        "        n_correct += torch.count_nonzero(targets == (outputs > 0.5)).item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%print_n_steps==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(str(_* train_params[\"batch_size\"]) + \"/\" + str(len(train_df)) + \" - Steps. Acc ->\", accu_step, \"Loss ->\", loss_step)\n",
        "            acc_step_holder.append(accu_step), loss_step_holder.append(loss_step)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "Gbi_w90332CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "GB023boK2Ghk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    #For each batch of data\n",
        "  for i in range(len(list_of_training_files)):\n",
        "\n",
        "\n",
        "    print('Batch', i, ':')\n",
        "\n",
        "    #Pull training dataframe\n",
        "    train_df = pd.read_pickle(list_of_training_files[i])\n",
        "\n",
        "    #Make it a class\n",
        "    training_set =  BatchedCasesData(train_df, tokenizer, MAX_LEN)\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "    train(epoch)\n",
        "    \n",
        "    torch.save(model.state_dict(), \"/content/gdrive/MyDrive/Thesis/Models/batch_label_state_dict.pt\")\n",
        "  \n",
        "    #save the model at each epoch just in case!\n",
        "  torch.save(model.state_dict(), \"/content/gdrive/MyDrive/Thesis/Models/batch_label_state_dict.pt\")\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print('Training Took:', total_time)"
      ],
      "metadata": {
        "id": "oM-tAVIG4Bw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,5))\n",
        "ax1.plot(acc_step_holder, label=\"Accuracy\")\n",
        "ax2.plot(loss_step_holder, label=\"Loss\")\n",
        "ax1.title.set_text(\"Accuracy\")\n",
        "ax2.title.set_text(\"Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GODkQ2N9aKTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/batch_training_accuracy.pickle'\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(acc_step_holder, handle)\n",
        "\n",
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/batch_training_loss.pickle'\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(loss_step_holder, handle)"
      ],
      "metadata": {
        "id": "I5BBxq3QaCjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate the Model"
      ],
      "metadata": {
        "id": "OzAQCgjP2Klp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, testing_loader):\n",
        "    model.eval()\n",
        "\n",
        "    n_correct = 0; n_wrong = 0; total = 0;  tr_loss = 0; nb_tr_steps = 0 ; nb_tr_examples = 0;\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0): \n",
        "            \n",
        "            doc_ids = data['doc_id'].to(device, dtype = torch.long)\n",
        "            doc_mask = data['doc_mask'].to(device, dtype = torch.long) \n",
        "            targets = data['targets'].to(device, dtype = torch.float)  \n",
        "\n",
        "            outputs = model(doc_ids, doc_mask) \n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            \n",
        "            n_correct += torch.count_nonzero(targets == (outputs > 0.5)).item()\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _%print_n_steps==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples \n",
        "                print(str(_* test_params[\"batch_size\"]) + \"/\" + str(len(train_df)) + \" - Steps. Acc ->\", accu_step, \"Loss ->\", loss_step)\n",
        "\n",
        "             \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu"
      ],
      "metadata": {
        "id": "l6P9lm6k8TTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "val_acc = []\n",
        "\n",
        "for i in range(len(list_of_testing_files)):\n",
        "\n",
        "    print('Batch', i, ':')\n",
        "\n",
        "    #Pull training dataframe\n",
        "    test_df = pd.read_pickle(list_of_testing_files[i])\n",
        "\n",
        "    #Make it a class\n",
        "    testing_set =  BatchedCasesData(test_df, tokenizer, MAX_LEN)\n",
        "\n",
        "    testing_loader = DataLoader(testing_set, **test_params)\n",
        "\n",
        "    acc = validate_model(model, testing_loader)\n",
        "    val_acc.append(acc)\n",
        "\n",
        "#print(\"Accuracy on test data = %0.2f%%\" % sum(val_acc) / len(val_acc))\n",
        "print(\"List of Batch Accuracies:\", val_acc)\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print('Validation took:', total_time)"
      ],
      "metadata": {
        "id": "OoaJ7hg_8nQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/validation_accuracy.pickle'\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(val_acc, handle)"
      ],
      "metadata": {
        "id": "eaX8WCLMajLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}