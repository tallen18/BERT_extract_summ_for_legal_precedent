{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Steps for Preprocessing Batch Model Data\n",
        "\n",
        "\n",
        "\n",
        "### 1.   Import the Data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> *  Need to import the batched data that is created in Sentence_Model_Preporcessor\n",
        "* The data should not be labelled, it should just be batched\n",
        "\n",
        "\n",
        "### 2.   Break the batches into sentences and label if a summary occurs within a batch.\n",
        "> * Save a dataframe with examples of batch, label (0 if summary is not contained within the label, 1 if the summary is contained within the label)\n",
        "\n",
        "### 3. Balance and Split Data\n",
        "> * Use undersampling to have 2/3 labels of 0 and 1/3 label of 1.\n",
        "* Split undersampled data into training and testing set (80% and 20% repspectively)\n",
        "* Save training and testing as pickle files\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kd8sdjK2vbz1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlzrmvcPvEnV"
      },
      "outputs": [],
      "source": [
        "def save_as_pickle(filename, data):\n",
        "  with open(filename, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(data, handle)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import time\n",
        "import pickle"
      ],
      "metadata": {
        "id": "t9E8cvRKvbnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pulling in batched data that was previously created"
      ],
      "metadata": {
        "id": "tPil6LHFwbhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_data = pd.read_pickle('/content/gdrive/MyDrive/Thesis/Data/case_data_with_512_batches.pickle')\n",
        "batched_data = batched_data.head(20000) #This line controls how many cases I am training on"
      ],
      "metadata": {
        "id": "sUrZyzRFwG_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches =  batched_data['512_batches']\n",
        "summaries = batched_data['summaries']"
      ],
      "metadata": {
        "id": "fQ5YdW60wJHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now going to check if a summary occurs in each batch"
      ],
      "metadata": {
        "id": "5panLZS9we-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cos_sim(summ, pred):\n",
        "  '''\n",
        "  Calculates the similarity between two sentences\n",
        "  '''\n",
        "  X = summ\n",
        "  Y = pred\n",
        "\n",
        "  X_list = word_tokenize(X) \n",
        "  Y_list = word_tokenize(Y)\n",
        "\n",
        "  sw = stopwords.words('english') \n",
        "  l1 =[];l2 =[]\n",
        "\n",
        "  # remove stop words from the string\n",
        "  X_set = {w for w in X_list if not w in sw} \n",
        "  Y_set = {w for w in Y_list if not w in sw}\n",
        "\n",
        "  # form a set containing keywords of both strings \n",
        "  rvector = X_set.union(Y_set) \n",
        "  for w in rvector:\n",
        "      if w in X_set: l1.append(1) # create a vector\n",
        "      else: l1.append(0)\n",
        "      if w in Y_set: l2.append(1)\n",
        "      else: l2.append(0)\n",
        "  c = 0\n",
        "\n",
        "  for i in range(len(rvector)):\n",
        "        c+= l1[i]*l2[i]\n",
        "\n",
        "  denominator = float((sum(l1)*sum(l2))**0.5)\n",
        "\n",
        "  if denominator == 0:\n",
        "    c = 0\n",
        "    denominator = 1\n",
        "\n",
        "  cosine = c / denominator\n",
        "\n",
        "  return cosine\n",
        "\n",
        "def is_sent_in_summ(b, relevant_summ):\n",
        "  batch_sent = sent_tokenize(b)\n",
        "  for b_1 in batch_sent:\n",
        "    for summ in relevant_summ:\n",
        "      if get_cos_sim(b_1, summ) >= 0.7:\n",
        "      #print('1.', s)\n",
        "      #print('2.', summ)\n",
        "        return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "-gs0TSP_wY6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intermediate_save(all_batches, contains_summ, i):\n",
        "  '''\n",
        "  saving the file so that we can save memory and if it crashes, we have something to work with\n",
        "  '''\n",
        "  input_dict = {'input_sentences': all_batches, 'contains_summ': contains_summ}\n",
        "  input_df = pd.DataFrame(data = input_dict)\n",
        "\n",
        "  output_file_name = '/content/gdrive/MyDrive/Thesis/Data/'+str(i)+'_unbalanced_batch_class_data.pickle'\n",
        "  with open(output_file_name, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(input_df, handle)\n",
        "\n",
        "  all_batches = []\n",
        "  contains_summ = []\n",
        "\n",
        "  print('New File Saved!')\n",
        "\n",
        "  return all_batches, contains_summ, output_file_name"
      ],
      "metadata": {
        "id": "guIskOUUwlud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "all_batches = []\n",
        "contains_summ = []\n",
        "total_sums = 0\n",
        "total_batches_with_sum = 0\n",
        "list_of_files = []\n",
        "\n",
        "for i in range(len(batches)):\n",
        "\n",
        "  if i % 2000 == 0 and i != 0:\n",
        "    print('Done with', i, 'cases.')\n",
        "    total_batches_with_sum += sum(contains_summ)\n",
        "    all_batches, contains_summ, output_file_name = intermediate_save(all_batches, contains_summ, i)\n",
        "    list_of_files.append(output_file_name)\n",
        "    save_as_pickle('/content/gdrive/MyDrive/Thesis/Data/batch_class_files_list', list_of_files)\n",
        "\n",
        "  one_text = batches[i]\n",
        "  summ_for_text = summaries[i]\n",
        "  total_sums += len(summ_for_text)\n",
        "\n",
        "  for b in one_text:\n",
        "\n",
        "    score = is_sent_in_summ(b, summ_for_text) #Takes in batch and determines if it contains an summary\n",
        "    contains_summ.append(score)\n",
        "    all_batches.append(b)\n",
        "\n",
        "\n",
        "all_batches, contains_summ, output_file_name = intermediate_save(all_batches, contains_summ, i)\n",
        "list_of_files.append(output_file_name)\n",
        "save_as_pickle('/content/gdrive/MyDrive/Thesis/Data/batch_class_files_list', list_of_files)\n",
        "  \n",
        "end_time = time.time()\n",
        "time_taken = end_time - start_time\n",
        "print('Time Taken', time_taken)\n",
        "\n",
        "print('Total amount of summaries', total_sums)\n",
        "print('Total number of batches with summaries', total_batches_with_sum)"
      ],
      "metadata": {
        "id": "Hr8zh59WwpU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now undersampling and splitting the data"
      ],
      "metadata": {
        "id": "UfsRQBNkw02c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def balance_and_split(data_name):\n",
        "  '''\n",
        "  Function that balances, splits and saves the data we gvie to it\n",
        "  '''\n",
        "  input_df = pd.read_pickle(data_name)\n",
        "\n",
        "  pos_df = input_df[input_df.contains_summ == 1]\n",
        "  neg_df = input_df[input_df.contains_summ == 0]\n",
        "\n",
        "  num_negative_examples = len(neg_df)\n",
        "  num_positive_examples = len(pos_df)\n",
        "\n",
        "  sub_neg_df = neg_df.sample(len(pos_df)*2)\n",
        "\n",
        "  reduced_neg_examples = len(sub_neg_df) #Tracking num of negative examples we now have\n",
        "  balanced_df = pd.concat([pos_df, sub_neg_df], axis=0)\n",
        "\n",
        "  total_num_examples = num_positive_examples + reduced_neg_examples\n",
        "\n",
        "  train_bdf, test_bdf = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "\n",
        "  train_file_name = data_name[:-28] + 'batch_training_bdf.pickle'\n",
        "  test_file_name = data_name[:-28] + 'batch_testing_bdf.pickle'\n",
        "\n",
        "  with open(train_file_name, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(train_bdf, handle)\n",
        "\n",
        "\n",
        "  with open(test_file_name, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(test_bdf, handle)\n",
        "\n",
        "  return train_file_name, test_file_name, total_num_examples, reduced_neg_examples, num_positive_examples, num_negative_examples"
      ],
      "metadata": {
        "id": "tywpvpbrw5AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_of_saved_file_list = '/content/gdrive/MyDrive/Thesis/Data/batch_class_files_list'\n",
        "saved_file_names = pd.read_pickle(name_of_saved_file_list)\n",
        "\n",
        "list_of_training_files = []\n",
        "list_of_testing_files = []\n",
        "balanced_total_examples = 0\n",
        "reduced_negative_examples = 0\n",
        "original_positive_examples = 0\n",
        "original_negative_examples = 0\n",
        "\n",
        "for data_name in saved_file_names:\n",
        "  train_file_name, test_file_name, total_num_examples, reduced_neg_examples, num_positive_examples, num_negative_examples = balance_and_split(data_name)\n",
        "\n",
        "  list_of_training_files.append(train_file_name)\n",
        "  list_of_testing_files.append(test_file_name)\n",
        "  balanced_total_examples += total_num_examples\n",
        "  reduced_negative_examples += reduced_neg_examples\n",
        "  original_positive_examples += num_positive_examples\n",
        "  original_negative_examples += num_negative_examples\n",
        "\n",
        "print('Original Number of negative examples:', original_negative_examples)\n",
        "print('Original Number of positive examples:', original_positive_examples)\n",
        "print('Original Total Number of examples:', original_negative_examples + original_positive_examples)\n",
        "print('Percentage of Examples that were negative:', round(original_negative_examples / (original_negative_examples + original_positive_examples), 3))\n",
        "\n",
        "print('After balancing...')\n",
        "\n",
        "print('Reduced Number of Negative examples:', reduced_negative_examples)\n",
        "print('Total Number of examples:', balanced_total_examples)\n",
        "\n",
        "print('Training Files:', list_of_training_files)\n",
        "print('Testing Files:', list_of_testing_files)"
      ],
      "metadata": {
        "id": "MvpTAKIzxI4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}