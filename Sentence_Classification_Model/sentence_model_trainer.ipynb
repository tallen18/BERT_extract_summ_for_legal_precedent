{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgjFz-MXKhrb"
      },
      "source": [
        "# Steps for Model Training\n",
        "\n",
        "### 1. Build Data Loader Class\n",
        "\n",
        "### 2. Build Model\n",
        "> * Make mean pooling helper function\n",
        "* Make Sentence Classifying Class that includes a dropout and dense layer on top of pretrained model\n",
        "* Define training function\n",
        "* Train the Model\n",
        "\n",
        "\n",
        "### 3. Run Validation on Test Set\n",
        "\n",
        "### 4. Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av6JP2ljKjtW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMW1Cf63LQnz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF0OiV9LLSYo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        " \n",
        "import pandas as pd \n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "sentenc_model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(sentenc_model_name) \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7WURbDhnER8"
      },
      "source": [
        "Now that we have multiple batches of data, I am going to concatenate the batches for training. If the memory storage is too large, then I may need to train in batches of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbjtT5denSJc"
      },
      "source": [
        "Concatenation first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jbHX7CvtyCd"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a manual way of creating the files we are going to look at for training and testing the model. You could also use the list of training examples and the list of testing examples created at the end of the \"SentenceModelPreprocessor\" Colab File."
      ],
      "metadata": {
        "id": "0qmNVJvLqxRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlHQSU4Uo90O"
      },
      "outputs": [],
      "source": [
        "folder_location = '/content/gdrive/MyDrive/Thesis/Data/TrainTestBinClass/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78KHkkBuptXx"
      },
      "outputs": [],
      "source": [
        "file_numbers = ['1000', '2000', '3000', '4000', '5000', '5999', '7000', '8000', '9000', '10000', '11000', '12000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YwB9sh8qEG1"
      },
      "outputs": [],
      "source": [
        "list_of_training_files = []\n",
        "list_of_testing_files = []\n",
        "\n",
        "for number in file_numbers:\n",
        "  train_file = folder_location+number+'_training_bdf.pickle'\n",
        "  test_file = folder_location+number+'_testing_bdf.pickle'\n",
        "  list_of_training_files.append(train_file)\n",
        "  list_of_testing_files.append(test_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rWUy30xLfeh"
      },
      "source": [
        "### 1. Build Data Loader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z06F9wWLlE-"
      },
      "outputs": [],
      "source": [
        "class BatchedCasesData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        sentence = str(self.data.iloc[index].input_sentences)\n",
        "        sentence = \" \".join(sentence.split())\n",
        "\n",
        "        document = str(self.data.iloc[index].input_texts)\n",
        "        document = \" \".join(document.split())\n",
        "\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            [sentence, document], \n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'sent_id': torch.tensor(ids[0], dtype=torch.long),\n",
        "            'doc_id': torch.tensor(ids[1], dtype=torch.long),\n",
        "            'sent_mask': torch.tensor(mask[0], dtype=torch.long),\n",
        "            'doc_mask': torch.tensor(mask[1], dtype=torch.long),\n",
        "            'targets': torch.tensor([self.data.iloc[index].labels], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUyJ2hbILo9J"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoiivqpfLuaL"
      },
      "source": [
        "Make mean pooling helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dOkzcx1LuwY"
      },
      "outputs": [],
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xJrq6xYL5iq"
      },
      "source": [
        "Make Sentence Classifying Class that includes a dropout and dense layer on top of pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI7QGvY9L53b"
      },
      "outputs": [],
      "source": [
        "class SentenceBertClass(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", in_features=768):\n",
        "        super(SentenceBertClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(model_name)\n",
        "        self.pre_classifier = torch.nn.Linear(in_features*3, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 1)\n",
        "        self.classifierSigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sent_ids, doc_ids, sent_mask, doc_mask):\n",
        "\n",
        "        sent_output = self.l1(input_ids=sent_ids, attention_mask=sent_mask) \n",
        "        sentence_embeddings = mean_pooling(sent_output, sent_mask) \n",
        "\n",
        "        doc_output = self.l1(input_ids=doc_ids, attention_mask=doc_mask) \n",
        "        doc_embeddings = mean_pooling(doc_output, doc_mask)\n",
        "\n",
        "        # elementwise product of sentence embs and doc embs\n",
        "        combined_features = sentence_embeddings * doc_embeddings  \n",
        "\n",
        "        # Concatenate input features and their elementwise product\n",
        "        concat_features = torch.cat((sentence_embeddings, doc_embeddings, combined_features), dim=1)   \n",
        "        \n",
        "        pooler = self.pre_classifier(concat_features) \n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        output = self.classifierSigmoid(output) \n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF2Do2A0L9k6"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "state_dict = torch.load('/content/gdrive/MyDrive/Thesis/Models/state_dict.pt')\n",
        "\n",
        "model = SentenceBertClass(model_name=sentenc_model_name)\n",
        "model.to(device);\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "loss_function = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLcDu_lYMDmA"
      },
      "source": [
        "Define training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrQsSXc8MEIU"
      },
      "outputs": [],
      "source": [
        "print_n_steps = 500\n",
        "EPOCHS = 2 \n",
        "acc_step_holder, loss_step_holder = [], []\n",
        "\n",
        "\n",
        "def train(epoch):    \n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        sent_ids = data['sent_id'].to(device, dtype = torch.long)\n",
        "        doc_ids = data['doc_id'].to(device, dtype = torch.long)\n",
        "        sent_mask = data['sent_mask'].to(device, dtype = torch.long)\n",
        "        doc_mask = data['doc_mask'].to(device, dtype = torch.long) \n",
        "        targets = data['targets'].to(device, dtype = torch.float)  \n",
        "\n",
        "        outputs = model(sent_ids, doc_ids, sent_mask, doc_mask) \n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item() \n",
        "        n_correct += torch.count_nonzero(targets == (outputs > 0.5)).item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%print_n_steps==0 and _ != 0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(str(_* train_params[\"batch_size\"]) + \"/\" + str(len(train_df)) + \" - Steps. Acc ->\", accu_step, \"Loss ->\", loss_step)\n",
        "            acc_step_holder.append(accu_step), loss_step_holder.append(loss_step)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch for Batch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss for Epoch for Batch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy for Epoch for Batch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC-QRTTcMVgF"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqq2zdSDRQja"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oJoLQzyMR-X"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  #For each batch of data\n",
        "  for i in range(len(list_of_training_files)):\n",
        "\n",
        "    print('Batch', i, ':')\n",
        "\n",
        "    #Pull training dataframe\n",
        "    train_df = pd.read_pickle(list_of_training_files[i])\n",
        "\n",
        "    #Make it a class\n",
        "    training_set =  BatchedCasesData(train_df, tokenizer, MAX_LEN)\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "    train(epoch)\n",
        "  \n",
        "  #save the model at each epoch just in case!\n",
        "  torch.save(model.state_dict(), \"/content/gdrive/MyDrive/Thesis/Models/state_dict.pt\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print('Training Took:', total_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j27HGhzxGxK"
      },
      "outputs": [],
      "source": [
        "#Saving the accuracy and loss so it can be analyzed further\n",
        "\n",
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/training_accuracy.pickle'\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(acc_step_holder, handle)\n",
        "\n",
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/training_loss.pickle'\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(loss_step_holder, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6ByaB6NMXF0"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,5))\n",
        "ax1.plot(acc_step_holder, label=\"Accuracy\")\n",
        "ax2.plot(loss_step_holder, label=\"Loss\")\n",
        "ax1.title.set_text(\"Accuracy\")\n",
        "ax2.title.set_text(\"Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTNTz2SZMZpY"
      },
      "source": [
        "### Run Validation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqTw1ELEMdVn"
      },
      "outputs": [],
      "source": [
        "def validate_model(model, testing_loader):\n",
        "    model.eval()\n",
        "\n",
        "    n_correct = 0; n_wrong = 0; total = 0;  tr_loss = 0; nb_tr_steps = 0 ; nb_tr_examples = 0;\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0): \n",
        "            \n",
        "            sent_ids = data['sent_id'].to(device, dtype = torch.long)\n",
        "            doc_ids = data['doc_id'].to(device, dtype = torch.long)\n",
        "            sent_mask = data['sent_mask'].to(device, dtype = torch.long)\n",
        "            doc_mask = data['doc_mask'].to(device, dtype = torch.long) \n",
        "            targets = data['targets'].to(device, dtype = torch.float)  \n",
        "\n",
        "            outputs = model(sent_ids, doc_ids, sent_mask, doc_mask) \n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            \n",
        "            n_correct += torch.count_nonzero(targets == (outputs > 0.5)).item()\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _%print_n_steps==0 and _!=0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples \n",
        "                print(str(_* test_params[\"batch_size\"]) + \"/\" + str(len(train_df)) + \" - Steps. Acc ->\", accu_step, \"Loss ->\", loss_step)\n",
        "\n",
        "             \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQO3HWnJMiI9"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "val_acc = []\n",
        "\n",
        "for i in range(len(list_of_testing_files)):\n",
        "\n",
        "    print('Batch', i, ':')\n",
        "\n",
        "    #Pull training dataframe\n",
        "    test_df = pd.read_pickle(list_of_testing_files[i])\n",
        "    print('Number of Examples:', len(test_df))\n",
        "\n",
        "    #Make it a class\n",
        "    testing_set =  BatchedCasesData(test_df, tokenizer, MAX_LEN)\n",
        "\n",
        "    testing_loader = DataLoader(testing_set, **test_params)\n",
        "\n",
        "    acc = validate_model(model, testing_loader)\n",
        "    val_acc.append(acc)\n",
        "\n",
        "print(\"Accuracy on test data = %0.2f%%\" % sum(val_acc) / len(val_acc))\n",
        "print(\"List of Batch Accuracies:\", val_acc)\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print('Validation took:', total_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9xM3jDUzDRK"
      },
      "outputs": [],
      "source": [
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/validation_accuracy.pickle'\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(val_acc, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRGjkDfJMj4-"
      },
      "source": [
        "### Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zpFGFlMMlNE"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/gdrive/MyDrive/Thesis/Models/state_dict.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}