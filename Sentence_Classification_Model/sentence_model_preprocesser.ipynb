{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGPQsFU_Q0z3"
      },
      "source": [
        "# Steps for Preprocessing Data\n",
        "\n",
        "\n",
        "\n",
        "### 1.   Import the Data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> *   Read through Summary Pickle File \n",
        "*   Read through Case Data file and create dictionary with ordered lists of case text and associated summary\n",
        "*   Create dataframe of with colums of the Case text and the Summary\n",
        "\n",
        "\n",
        "### 2.   Build Batching Algorithm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> For each case text do the following:\n",
        "*   Break into Paragraphs.\n",
        "*   Check if the number of tokens in the paragraph is less than the minimum accepted paragraph length, if so, do not include it.\n",
        "* Check if the number of tokens in the paragraph is more than the maximum accepted amount, if so, send that paragraph to the \"Long Paragraph Function.\" (See Below)\n",
        "* If the number of tokens in the paragraph is larger than the mimumum accepted amount and smaller than the maximum accepted amount, add the paragraph to the batch and go to the \"Next Paragraph\" fuction to see if the next paragraph can be added to the batch. (See Below)\n",
        "* Pull all the batches together and create a list of batched text for the case.\n",
        "\n",
        "\n",
        "> Long Paragraph Function:\n",
        "* Calculate a target number of batches based on the length of the paragraph.\n",
        "* Calculate target number of tokens per batch based on the number of batches and length of paragraph. (ensures a more even number of tokens per batch)\n",
        "* Split the paragraph into sentences.\n",
        "* If a sentence exists that is larger than the maximum allowed tokens, print that sentence but do not consider it for a batch.\n",
        "* Otherwise, add sentences to the batch until reaching the target number of tokens per batch.\n",
        "* Start a new batch and add sentences again until entire paragraph is accurately batched.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Next Paragraph Function\n",
        "* Looks at the length of the next paragraph, if it above the minimum token length and can be added to the previous paragraph without exceeding the maximum token length, it is added to the batch.\n",
        "\n",
        "### 3. Run batching algorithm and save as a new column in the dataframe\n",
        "\n",
        "\n",
        "\n",
        "### 4.   Break all data into sentences and label which sentences are in the summary.\n",
        "> * Create helper functions to determine if two sentences are the same.\n",
        "* Break batches into sentences, compare each sentences to the sentences in the gold summary, and label as 1 if the sentence exists in the gold summary\n",
        "* Create a new dataframe storing sentence, batch, label and save as a pickle file.\n",
        "\n",
        "### 5. Balance and Split Data\n",
        "> * Use undersampling to have 2/3 labels of 0 and 1/3 label of 1.\n",
        "* Split undersampled data into training and testing set (80% and 20% repspectively)\n",
        "* Save training and testing as pickle files\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gheb6d4fQXYN"
      },
      "source": [
        "### 1. Import the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8HA9gtVQMs1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbfF-GBg41cY"
      },
      "source": [
        "Read through Summary Pickle File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy_k7Xep4yoZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import lzma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivWdqWhgDaDr"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcc8JM0P00C9"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n0bF8-M5a8Y"
      },
      "outputs": [],
      "source": [
        "summary_file = '/content/gdrive/MyDrive/Thesis/Data/unique_citations_origin_dict.pickle'\n",
        "all_opinion_data = '/content/gdrive/MyDrive/Thesis/Data/case_data.jsonl.xz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kHyJKDD5dSr"
      },
      "outputs": [],
      "source": [
        "def read_through_pickle(file):\n",
        "\t'''\n",
        "\tThis function just takes in a pickle file and reads it. Currently just returns the dictionary from my pickle file\n",
        "\t'''\n",
        "\tobjects = []\n",
        "\twith (open(file, 'rb')) as openfile:\n",
        "\t\twhile True:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tunique_citations = pickle.load(openfile)\n",
        "\t\t\t\treturn unique_citations #This is just returning the file\n",
        "\t\t\t\tbreak\n",
        "\t\t\texcept EOFError:\n",
        "\t\t\t\treturn \"There was an Error Loading Pickle File!!!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43prTrwLDteT"
      },
      "outputs": [],
      "source": [
        "def make_summaries_sentences(summaries):\n",
        "  '''\n",
        "  This helper function inputs a list of summaries and returns a list of all the setences included in the summaries\n",
        "  '''\n",
        "  list_of_sentences = []\n",
        "  for summ in summaries:\n",
        "    sentences = sent_tokenize(summ)\n",
        "    list_of_sentences += sentences\n",
        "\n",
        "  return list_of_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeysBOhx5r5n"
      },
      "outputs": [],
      "source": [
        "id_to_sum = read_through_pickle(summary_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ87H6D051If"
      },
      "source": [
        "Read through Case Data file and create dictionary with ordered lists of case text and associated summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exOXBRcK54nw"
      },
      "outputs": [],
      "source": [
        "df_dict = {'case_text': [], 'summaries': []} #Will be {case_text: ____, summaries: _____}\n",
        "count = 0\n",
        "with lzma.open(all_opinion_data) as in_file:\n",
        "  for i, line in enumerate(in_file):\n",
        "\n",
        "    #This line controls how many cases I am going to be training with\n",
        "    if count >= 350000:\n",
        "      break\n",
        "    \n",
        "    else:\n",
        "\n",
        "      cases = json.loads(str(line,'utf8'))\n",
        "      case_id = cases['id']\n",
        "\n",
        "      if case_id in id_to_sum:\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        case_text = cases['casebody']['data']['opinions'][0]['text']\n",
        "        case_summaries = [x[1] for x in id_to_sum[case_id]]\n",
        "\n",
        "        df_dict['case_text'].append(case_text)\n",
        "        df_dict['summaries'].append(make_summaries_sentences(case_summaries)) #Making sure the summaries consist of a list of individual sentences\n",
        "\n",
        "  print(\"We are are analyzing \", count, \" summaries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smCejscn6BVu"
      },
      "source": [
        "Create dataframe of with colums of the Case text and the Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUgWFT_t5gPz"
      },
      "outputs": [],
      "source": [
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/case_data_no_batches.pickle'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPB8vZHo6CE1"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data = df_dict)\n",
        "\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(df, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO4vsKYm6UBs"
      },
      "source": [
        "### 2. Build Batching Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7WcA1TI6c7_"
      },
      "source": [
        "> For each case text do the following:\n",
        "*   Break into Paragraphs.\n",
        "*   Check if the number of tokens in the paragraph is less than the minimum accepted paragraph length, if so, do not include it.\n",
        "* Check if the number of tokens in the paragraph is more than the maximum accepted amount, if so, send that paragraph to the \"Long Paragraph Function.\" (See Below)\n",
        "* If the number of tokens in the paragraph is larger than the mimumum accepted amount and smaller than the maximum accepted amount, add the paragraph to the batch and go to the \"Next Paragraph\" fuction to see if the next paragraph can be added to the batch. (See Below)\n",
        "* Pull all the batches together and create a list of batched text for the case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mnUsNB__FZr"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksLye24h6cta"
      },
      "outputs": [],
      "source": [
        "model_name = \"bert-base-uncased\" #Change this to use a different model\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntmMEgpe6YPr"
      },
      "outputs": [],
      "source": [
        "def create_case_batches(case_text, min_length = 15, max_tokens = 512):\n",
        "\n",
        "  #Break into paragraphs\n",
        "  split_text = case_text.split('\\n')\n",
        "\n",
        "  batches = []\n",
        "\n",
        "  used_indices = set() #Keeps track of the paragraphs we have already used\n",
        "\n",
        "  for i in range(len(split_text)):\n",
        "\n",
        "    single_batch = ''\n",
        "    \n",
        "    if i not in used_indices:\n",
        "      paragraph = split_text[i]\n",
        "      num_tokens = len(tokenizer.tokenize(paragraph))\n",
        "\n",
        "\n",
        "      #Taking out the small paragraphs\n",
        "      if num_tokens > min_length: \n",
        "        \n",
        "        #Sending long paragraphs to a new batching algorithm\n",
        "        if num_tokens > max_tokens:\n",
        "          batches_from_long_p = batch_long_paragraphs(paragraph, num_tokens, max_tokens)\n",
        "          batches += batches_from_long_p\n",
        "        \n",
        "        #Sending paragraphs that are less than the max to \n",
        "        else:\n",
        "          single_batch += paragraph\n",
        "          tokens_used = num_tokens\n",
        "          updated_single_batch, used_indices = get_next_paragraph(split_text, i, single_batch, tokens_used, max_tokens, used_indices, min_length)\n",
        "          batches.append(updated_single_batch)\n",
        "\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ziien3V26wlU"
      },
      "source": [
        "> Long Paragraph Function:\n",
        "* Calculate a target number of batches based on the length of the paragraph.\n",
        "* Calculate target number of tokens per batch based on the number of batches and length of paragraph. (ensures a more even number of tokens per batch)\n",
        "* Split the paragraph into sentences.\n",
        "* If a sentence exists that is larger than the maximum allowed tokens, print that sentence but do not consider it for a batch.\n",
        "* Otherwise, add sentences to the batch until reaching the target number of tokens per batch.\n",
        "* Start a new batch and add sentences again until entire paragraph is accurately batched.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6CzMaBq60-I"
      },
      "outputs": [],
      "source": [
        "def batch_long_paragraphs(paragraph, num_tokens, max_tokens):\n",
        "  '''\n",
        "  The inputs for this helper function is a paragraph longer than the max number of tokens\n",
        "  It will break up the paragraphs into groups of sentences as evenly spaced as possible\n",
        "  '''\n",
        "  num_batches = (num_tokens // max_tokens) + 1 # How many batches are needed for the paragraph\n",
        "  target_batch_tokens = max_tokens // num_batches # How many tokens we are looking to have in each batch\n",
        "\n",
        "  paragraph_sentences = sent_tokenize(paragraph)\n",
        "  \n",
        "  tokens_used = 0\n",
        "  current_batch = ''\n",
        "\n",
        "  longp_batches = []\n",
        "\n",
        "  for i in range(len(paragraph_sentences)):\n",
        "\n",
        "    sentence = paragraph_sentences[i]\n",
        "    num_tokens_in_sent = len(tokenizer.tokenize(sentence))\n",
        "\n",
        "    if num_tokens_in_sent > max_tokens:\n",
        "      longp_batches += batch_long_sentences(sentence, num_tokens, max_tokens)\n",
        "    \n",
        "    else:\n",
        "\n",
        "      if tokens_used + num_tokens_in_sent <= target_batch_tokens + 10: #Adding in a buffer\n",
        "        tokens_used += num_tokens_in_sent\n",
        "        current_batch += ' ' + sentence\n",
        "      \n",
        "      else:\n",
        "        longp_batches.append(current_batch)\n",
        "        tokens_used = num_tokens_in_sent\n",
        "        current_batch = sentence\n",
        "\n",
        "  return longp_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MbzTaGz5hin"
      },
      "source": [
        "  > Long Sentence Function\n",
        "* Same funcunality as the long paragraph function except that it breaks down words instead of sentences and continuously adds them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgMNbZwb5flQ"
      },
      "outputs": [],
      "source": [
        "def batch_long_sentences(sentence, num_tokens, max_tokens):\n",
        "\n",
        "  num_batches = (num_tokens // (max_tokens - 2)) + 1 # How many batches are needed for the paragraph\n",
        "  target_batch_tokens = (max_tokens - 2) // num_batches # How many tokens we are looking to have in each batch\n",
        "\n",
        "  sentence_words = word_tokenize(sentence)\n",
        "\n",
        "  tokens_used = 0\n",
        "  current_batch = ''\n",
        "\n",
        "  longs_batches = []\n",
        "\n",
        "  for i in range(len(sentence_words)):\n",
        "\n",
        "      word = sentence_words[i]\n",
        "      num_tokens_in_word = len(tokenizer.tokenize(word, add_special_tokens = False))\n",
        "\n",
        "      if num_tokens_in_word > max_tokens - 2:\n",
        "        print('THERE EXISTS A WORD LONGER THAN THE MAX TOKENS')\n",
        "        print(word)\n",
        "      \n",
        "      else:\n",
        "\n",
        "        if tokens_used + num_tokens_in_word <= target_batch_tokens + 5: #Adding in a buffer\n",
        "          tokens_used += num_tokens_in_word\n",
        "          current_batch += ' ' + word\n",
        "        \n",
        "        else:\n",
        "          if tokens_used > 15:\n",
        "            longs_batches.append(current_batch)\n",
        "          tokens_used = num_tokens_in_word\n",
        "          current_batch = word\n",
        "\n",
        "  return longs_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwbHQjM_64tA"
      },
      "source": [
        "  > Next Paragraph Function\n",
        "* Looks at the length of the next paragraph, if it above the minimum token length and can be added to the previous paragraph without exceeding the maximum token length, it is added to the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlNo0xxQ7EHW"
      },
      "outputs": [],
      "source": [
        "def get_next_paragraph(split_text, i, single_batch, tokens_used, max_tokens, used_indices, min_tokens):\n",
        "  '''\n",
        "  Recursive Helper Function to add paragraphs until maxing out the tokens\n",
        "  '''\n",
        "  next_index = i+1\n",
        "\n",
        "  #The current paragraph is the last one\n",
        "  if next_index >= len(split_text): \n",
        "    return single_batch, used_indices\n",
        "\n",
        "  next_paragraph = split_text[next_index]\n",
        "\n",
        "  num_next_tokens = len(tokenizer.tokenize(next_paragraph))\n",
        "\n",
        "  if num_next_tokens > min_tokens: #Getting rid of the small paragraphs\n",
        "\n",
        "    #Making sure that adding the next paragraph keeps it below max_tokens tokens\n",
        "    if tokens_used + num_next_tokens > max_tokens:\n",
        "      return single_batch, used_indices\n",
        "\n",
        "    #Adding the next paragraph to the batch and updating indices and tokens\n",
        "    else:\n",
        "      single_batch += ' ' + next_paragraph\n",
        "      used_indices.add(next_index)\n",
        "      tokens_used += num_next_tokens\n",
        "      return get_next_paragraph(split_text, next_index, single_batch, tokens_used, max_tokens, used_indices, min_tokens)\n",
        "  \n",
        "  else:\n",
        "    used_indices.add(next_index)\n",
        "    return get_next_paragraph(split_text, next_index, single_batch, tokens_used, max_tokens, used_indices, min_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTY0W7x47u6v"
      },
      "source": [
        "### 3.   Run Batching Algorithm and save as new column in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pqpVJfe7wAg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle('/content/gdrive/MyDrive/Thesis/Data/case_data_no_batches.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKTiBAlk8EEM"
      },
      "outputs": [],
      "source": [
        "case_texts = df['case_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the batching algorithm and save it into the dataframe"
      ],
      "metadata": {
        "id": "eYYkfh1QnaFa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo59W2Jx8Npc"
      },
      "outputs": [],
      "source": [
        "all_512_batched_text = []\n",
        "for case_text in case_texts:\n",
        "  batched_text = create_case_batches(case_text, 15, 512)\n",
        "  all_512_batched_text.append(batched_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGIQCFbo8U7a"
      },
      "outputs": [],
      "source": [
        "df['512_batches'] = all_512_batched_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLqulegQ9SLR"
      },
      "outputs": [],
      "source": [
        "output_file = '/content/gdrive/MyDrive/Thesis/Data/case_data_with_512_batches.pickle'\n",
        "with open(output_file, 'wb') as handle:   #Saving as a pickle file\n",
        "  pickle.dump(df, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaJZiQvH6B5M"
      },
      "source": [
        "Visualizing the length of the batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1wxrLyC6HVb"
      },
      "outputs": [],
      "source": [
        "all_tokens = []\n",
        "for batches in all_512_batched_text:\n",
        "  for batch in batches:\n",
        "    num_tokens = len(tokenizer.tokenize(batch))\n",
        "    all_tokens.append(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV0TSCZI6JtG"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "print(len(all_tokens))\n",
        "max_length = max(all_tokens)\n",
        "min_length = min(all_tokens)\n",
        "print('Smallest Paragraph Length is', min_length)\n",
        "\n",
        "bins = [x for x in range(min_length, max_length, int(max_length / 20))]\n",
        "#print(bins)\n",
        "plt.hist(all_tokens, bins = bins)\n",
        "plt.axvline(x=401.978, color='red')\n",
        "# Add a label to the line\n",
        "plt.text(390, plt.ylim()[1]*0.9, 'Average = 401.98', ha='right', va='center', color='red')\n",
        "plt.xlabel('Number of Tokens per Batch')\n",
        "plt.ylabel('Number of Batches')\n",
        "plt.title('Length of Batches after Batching Algorithm')\n",
        "print(\"Max Paragraph Length is\", max_length)\n",
        "\n",
        "print('Average Paragrah Length is', sum(all_tokens) / len(all_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB8NLz6jGJnO"
      },
      "source": [
        "### 4.   Break all data into sentences and label which sentences are in the summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci9HL_FCGTNS"
      },
      "source": [
        "Create helper functions to determine if two sentences are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MoH6DW8GPE9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUDFyLTNG1y3"
      },
      "outputs": [],
      "source": [
        "def get_cos_sim(summ, pred):\n",
        "  '''\n",
        "  Calculates the similarity between two sentences\n",
        "  '''\n",
        "  X = summ\n",
        "  Y = pred\n",
        "\n",
        "  X_list = word_tokenize(X) \n",
        "  Y_list = word_tokenize(Y)\n",
        "\n",
        "  sw = stopwords.words('english') \n",
        "  l1 =[];l2 =[]\n",
        "\n",
        "  # remove stop words from the string\n",
        "  X_set = {w for w in X_list if not w in sw} \n",
        "  Y_set = {w for w in Y_list if not w in sw}\n",
        "\n",
        "  # form a set containing keywords of both strings \n",
        "  rvector = X_set.union(Y_set) \n",
        "  for w in rvector:\n",
        "      if w in X_set: l1.append(1) # create a vector\n",
        "      else: l1.append(0)\n",
        "      if w in Y_set: l2.append(1)\n",
        "      else: l2.append(0)\n",
        "  c = 0\n",
        "\n",
        "  for i in range(len(rvector)):\n",
        "        c+= l1[i]*l2[i]\n",
        "\n",
        "  denominator = float((sum(l1)*sum(l2))**0.5)\n",
        "\n",
        "  if denominator == 0:\n",
        "    c = 0\n",
        "    denominator = 1\n",
        "\n",
        "  cosine = c / denominator\n",
        "\n",
        "  return cosine\n",
        "\n",
        "def is_sent_in_summ(s, summ):\n",
        "  '''\n",
        "  takes in a single sentence and a list of sentneces and detemrines if that single sentence is included in the list of sentences\n",
        "  '''\n",
        "  for summ in relevant_summ:\n",
        "    if get_cos_sim(s, summ) >= 0.7:\n",
        "      #print('1.', s)\n",
        "      #print('2.', summ)\n",
        "      return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A_mXfw7HKTL"
      },
      "source": [
        "Break batches into sentences, compare each sentences to the sentences in the gold summary, and label as 1 if the sentence exists in the gold summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl9PXPJAG7FI"
      },
      "outputs": [],
      "source": [
        "batched_df = pd.read_pickle('/content/gdrive/MyDrive/Thesis/Data/case_data_with_512_batches.pickle')\n",
        "\n",
        "case_text = batched_df['case_text']\n",
        "summaries = batched_df['summaries']\n",
        "batches = batched_df['512_batches']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTxhPeVu2-qF"
      },
      "outputs": [],
      "source": [
        "def intermediate_save(input_sentences, input_texts, labels, i):\n",
        "  '''\n",
        "  saving the file so that we can save memory and if it crashes, we have something to work with\n",
        "  '''\n",
        "  input_dict = {'input_sentences': input_sentences, 'input_texts': input_texts, 'labels': labels}\n",
        "  input_df = pd.DataFrame(data = input_dict)\n",
        "\n",
        "  output_file_name = '/content/gdrive/MyDrive/Thesis/Data/'+str(i)+'_unbalanced_input_data.pickle'\n",
        "  with open(output_file_name, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(input_df, handle)\n",
        "\n",
        "  input_sentences = []\n",
        "  input_texts = []\n",
        "  labels = []\n",
        "\n",
        "  return input_sentences, input_texts, labels, output_file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVWdmIxR9VEm"
      },
      "outputs": [],
      "source": [
        "def analyze_batch(passage, relevant_summ):\n",
        "  '''\n",
        "  This helper function takes in a batch of text as a string and a list of \"golden sentences\" from a specific case\n",
        "  and returns the sentences from the batch, a list of the batches to go with each sentence, a list of labels that say whether or not\n",
        "  the sentence from the batch is a golden sentence, and a counter for the total number of golden sentences found\n",
        "  '''\n",
        "\n",
        "  input_sentences = []\n",
        "  input_texts = []\n",
        "  labels = []\n",
        "  total_yes_labels = 0\n",
        "\n",
        "  passage_sents = sent_tokenize(passage)\n",
        "  input_sentences.append(passage_sents)\n",
        "\n",
        "  for s in passage_sents:\n",
        "\n",
        "    input_sentences.append(s) #adding the sentence to the new list of data\n",
        "    input_texts.append(passage) #adding the passage to the new list of data\n",
        "\n",
        "    if is_sent_in_summ(s, relevant_summ): #Use helper to calculate whether the sentence from the batch is included in the summary\n",
        "        label = 1\n",
        "        total_yes_labels += 1\n",
        "    \n",
        "    else:\n",
        "        label = 0\n",
        "    \n",
        "    labels.append(label)\n",
        "  \n",
        "  labels.append(holder)\n",
        "  \n",
        "  return input_sentences, input_texts, labels, total_yes_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aBgss4JAI-0"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code creates a list of all sentences from every opinion, a list of the batches those sentences come from, and a label (0 or 1) as to whether that sentence is a golden sentence.\n",
        "\n",
        "It also saves throughout the process in case of failures and tracks key statisitcs about the dataset we are using."
      ],
      "metadata": {
        "id": "hx1yG3sloVPj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE3Ylz_ZHB-V"
      },
      "outputs": [],
      "source": [
        "input_sentences = []\n",
        "input_texts = []\n",
        "labels = []\n",
        "saved_output_files = []\n",
        "saved_file_names = []\n",
        "output_file_name = None\n",
        "list_file_name = '/content/gdrive/MyDrive/Thesis/Data/saved_file_names.pickle'\n",
        "\n",
        "#Keeping track of passgaes, summaries and labels to know how many we have\n",
        "unique_passage_count = 0\n",
        "total_summary_sentences = 0\n",
        "total_yes_labels = 0\n",
        "time_for_labeling = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(len(case_text)):\n",
        "\n",
        "  passage_sents_holder = []\n",
        "  sent_label_holder = []\n",
        "  contains_summ_holder = []\n",
        "\n",
        "  if i % 500 == 0 and i != 0:\n",
        "    print(i,'/', len(case_text))\n",
        "\n",
        "    #Do this step to make sure I have progress throughout labelling\n",
        "    input_sentences, input_texts, labels, output_file_name = intermediate_save(input_sentences, input_texts, labels, i)\n",
        "    saved_file_names.append(output_file_name)\n",
        "\n",
        "    #Keeps track of all the saved files so I can use them for training\n",
        "    with open(list_file_name, 'wb') as handle:   #Saving as a pickle file\n",
        "      pickle.dump(saved_file_names, handle)\n",
        "\n",
        "    print('New File Saved')\n",
        "\n",
        "  relevant_summ = list(summaries[i]) #Pull summaries for each case\n",
        "  relevant_batches = list(batches[i]) #Pull batches for each case\n",
        "\n",
        "  unique_passage_count += len(relevant_batches)\n",
        "  total_summary_sentences += len(relevant_summ)\n",
        "\n",
        "  for passage in relevant_batches: #For each batch\n",
        "\n",
        "    contains_summ = 0\n",
        "\n",
        "    passage_sents = sent_tokenize(passage) #Extract the sentences from each batch\n",
        "\n",
        "    passage_sents_holder.append(passage_sents) #Add sentences of\n",
        "\n",
        "    batch_sents_label = [] \n",
        "\n",
        "    for s in passage_sents:\n",
        "\n",
        "      input_sentences.append(s) #adding the sentence to the new list of data\n",
        "      input_texts.append(passage) #adding the passage to the new list of data\n",
        "\n",
        "      if is_sent_in_summ(s, relevant_summ): #Use helper to calculate whether the sentence from the batch is included in the summary\n",
        "        label = 1\n",
        "        total_yes_labels += 1\n",
        "      \n",
        "      else:\n",
        "        label = 0\n",
        "\n",
        "      labels.append(label)\n",
        "\n",
        "input_sentences, input_texts, labels, output_file_name = intermediate_save(input_sentences, input_texts, labels, i)\n",
        "saved_file_names.append(output_file_name)\n",
        "\n",
        "output_name = '/content/gdrive/MyDrive/Thesis/Data/aggregation_data.pickle'\n",
        "with open(output_name, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(batched_df, handle)\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(\"Time taken:\", total_time, \"seconds\")\n",
        "#print(time_for_labeling)\n",
        "\n",
        "print('Number of Sentences:', len(input_sentences))\n",
        "print('Number of Summary Sentences:', total_summary_sentences)\n",
        "print('Number of yes labels:', total_yes_labels)\n",
        "print('ABOVE TWO NUMBERS SHOULD BE ROUGHLY EQUAL')\n",
        "print('Number of unique passages', unique_passage_count)\n",
        "\n",
        "#print(len(saved_file_names))\n",
        "#print(saved_file_names)\n",
        "#print(pd.read_pickle(list_file_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLNF4DOjJM1j"
      },
      "source": [
        "### 5. Balance and Split Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tt6aaWNJRNt"
      },
      "source": [
        "Use undersampling to have 2/3 labels of 0 and 1/3 label of 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWdNTLt7DcOu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def balance_and_split(data_name):\n",
        "  '''\n",
        "  Function that balances, splits and saves the data we gvie to it\n",
        "  '''\n",
        "  input_df = pd.read_pickle(data_name)\n",
        "\n",
        "  pos_df = input_df[input_df.labels == 1]\n",
        "  neg_df = input_df[input_df.labels == 0]\n",
        "\n",
        "  num_negative_examples = len(neg_df)\n",
        "  num_positive_examples = len(pos_df)\n",
        "\n",
        "  sub_neg_df = neg_df.sample(len(pos_df)*2)\n",
        "\n",
        "  reduced_neg_examples = len(sub_neg_df) #Tracking num of negative examples we now have\n",
        "  balanced_df = pd.concat([pos_df, sub_neg_df], axis=0)\n",
        "\n",
        "  total_num_examples = num_positive_examples + reduced_neg_examples\n",
        "\n",
        "  train_bdf, test_bdf = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "\n",
        "  train_file_name = data_name[:-28] + 'training_bdf.pickle'\n",
        "  test_file_name = data_name[:-28] + 'testing_bdf.pickle'\n",
        "\n",
        "  with open(train_file_name, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(train_bdf, handle)\n",
        "\n",
        "\n",
        "  with open(test_file_name, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(test_bdf, handle)\n",
        "\n",
        "  return train_file_name, test_file_name, total_num_examples, reduced_neg_examples, num_positive_examples, num_negative_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NwRAoLAJLUm"
      },
      "outputs": [],
      "source": [
        "name_of_saved_file_list = '/content/gdrive/MyDrive/Thesis/Data/saved_file_names.pickle'\n",
        "saved_file_names = pd.read_pickle(name_of_saved_file_list)\n",
        "\n",
        "list_of_training_files = []\n",
        "list_of_testing_files = []\n",
        "balanced_total_examples = 0\n",
        "reduced_negative_examples = 0\n",
        "original_positive_examples = 0\n",
        "original_negative_examples = 0\n",
        "\n",
        "for data_name in saved_file_names:\n",
        "  train_file_name, test_file_name, total_num_examples, reduced_neg_examples, num_positive_examples, num_negative_examples = balance_and_split(data_name)\n",
        "\n",
        "  list_of_training_files.append(train_file_name)\n",
        "  list_of_testing_files.append(test_file_name)\n",
        "  balanced_total_examples += total_num_examples\n",
        "  reduced_negative_examples += reduced_neg_examples\n",
        "  original_positive_examples += num_positive_examples\n",
        "  original_negative_examples += num_negative_examples\n",
        "\n",
        "print('Original Number of negative examples:', original_negative_examples)\n",
        "print('Original Number of positive examples:', original_positive_examples)\n",
        "print('Original Total Number of examples:', original_negative_examples + original_positive_examples)\n",
        "print('Percentage of Examples that were negative:', round(original_negative_examples / (original_negative_examples + original_positive_examples), 3))\n",
        "\n",
        "print('After balancing...')\n",
        "\n",
        "print('Reduced Number of Negative examples:', reduced_negative_examples)\n",
        "print('Total Number of examples:', balanced_total_examples)\n",
        "\n",
        "print('Training Files:', list_of_training_files)\n",
        "print('Testing Files:', list_of_testing_files)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}