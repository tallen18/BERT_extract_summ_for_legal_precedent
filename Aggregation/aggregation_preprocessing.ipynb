{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDhXJQdFwpIY"
      },
      "source": [
        "# Steps for Preprocessing Aggregation Data\n",
        "\n",
        "### 1. Calculate the predcited scores from the sentence classification model for every sentence in a case\n",
        "> * Need a dataset woth colums of every case, every batch, every sentencece from every batch, the summaries from a case, and 0 / 1 labels relating to every sentence in every batch that says whether or not a sentence is in the summary.\n",
        "* Need to upload the state dict from the trained model\n",
        "* Just run the data through the trained model (Takes a very long time)\n",
        "\n",
        "### 2. Calculate the positional weight for each batch and create a new column that mulitplies every score by the positional weight of its batch\n",
        "\n",
        "### 3. Calculate the positional weight for each batch and create a new column that mulitplies every score by the positional weight of its batch\n",
        ">* Need the state dict from the trained batch classification model. Calculated the predicted output for each batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Nmfpunw4js"
      },
      "source": [
        "Uploading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiLVtpflwmMj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZomN3P3cKhPb"
      },
      "outputs": [],
      "source": [
        "def save_as_pickle(filename, data):\n",
        "  with open(filename, 'wb') as handle:   #Saving as a pickle file\n",
        "    pickle.dump(data, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqg0Rq8k00Si"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkpmAdne2k1C"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers  rouge-score sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al-1iXbr2sAM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        " \n",
        "import plotly.express as px\n",
        "\n",
        "import os \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        " \n",
        "from transformers import AutoTokenizer, AutoModel \n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH97OQ3-xEux"
      },
      "outputs": [],
      "source": [
        "batched_df = pd.read_pickle('/content/gdrive/MyDrive/Thesis/Data/aggregation_data.pickle')\n",
        "\n",
        "smaller_df = batched_df.head(300) #NEEDED TO CUT OFF NUMBER OF CASES ANALYZING FOR COMPLEXITY REASONS\n",
        "\n",
        "state_dict = torch.load(\"/content/gdrive/MyDrive/Thesis/Models/state_dict.pt\", map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABMIY880wKX"
      },
      "source": [
        "### 1. Calculate the predcited scores from the sentence classification model for every sentence in a case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLWWrKUN2s_J"
      },
      "outputs": [],
      "source": [
        "# get mean pooling for sentence bert models \n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class SentenceBertClass(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", in_features=768):\n",
        "        super(SentenceBertClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(model_name)\n",
        "        self.pre_classifier = torch.nn.Linear(in_features*3, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 1)\n",
        "        self.classifierSigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sent_ids, doc_ids, sent_mask, doc_mask):\n",
        "\n",
        "        sent_output = self.l1(input_ids=sent_ids, attention_mask=sent_mask) \n",
        "        sentence_embeddings = mean_pooling(sent_output, sent_mask) \n",
        "\n",
        "        doc_output = self.l1(input_ids=doc_ids, attention_mask=doc_mask) \n",
        "        doc_embeddings = mean_pooling(doc_output, doc_mask)\n",
        "\n",
        "        # elementwise product of sentence embs and doc embs\n",
        "        combined_features = sentence_embeddings * doc_embeddings  \n",
        "\n",
        "        # Concatenate input features and their elementwise product\n",
        "        concat_features = torch.cat((sentence_embeddings, doc_embeddings, combined_features), dim=1)   \n",
        "        \n",
        "        pooler = self.pre_classifier(concat_features) \n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        output = self.classifierSigmoid(output) \n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5Z7glsN2y30"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "extractive_model = SentenceBertClass() \n",
        "extractive_model.load_state_dict(state_dict)\n",
        "extractive_model.eval(); "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bznIPUt3Py4"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# tokenize text as required by BERT based models\n",
        "def get_tokens(text, tokenizer):\n",
        "  inputs = tokenizer.batch_encode_plus(\n",
        "            text, \n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "  ids = inputs['input_ids']\n",
        "  mask = inputs['attention_mask']\n",
        "  return ids, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxbuchOI3p3p"
      },
      "outputs": [],
      "source": [
        "# get predictions given some an array of sentences and their corresponding documents\n",
        "def predict(model,sents, doc):\n",
        "  sent_id, sent_mask = get_tokens(sents,tokenizer)\n",
        "  sent_id, sent_mask = torch.tensor(sent_id, dtype=torch.long),torch.tensor(sent_mask, dtype=torch.long)\n",
        " \n",
        "  doc_id, doc_mask = get_tokens([doc],tokenizer)\n",
        "  doc_id, doc_mask = doc_id * len(sents), doc_mask* len(sents)\n",
        "  doc_id, doc_mask = torch.tensor(doc_id, dtype=torch.long),torch.tensor(doc_mask, dtype=torch.long)\n",
        "\n",
        "  preds = model(sent_id, doc_id, sent_mask, doc_mask)\n",
        "  return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb2sHdbO3x6C"
      },
      "outputs": [],
      "source": [
        "def batch_and_score(doc, model, doc_sentences, batch_size=3):\n",
        "  '''\n",
        "  All this function does is take in a document, a model, the list of sentences, it then batches the sentence and sends it though the predictor to generatre prediction scores\n",
        "  '''\n",
        "  scores = [] \n",
        "\n",
        "  # run predictions using some batch size\n",
        "  for i in range(int(len(doc_sentences) / batch_size) + 1):\n",
        "    batch_start = i*batch_size  \n",
        "    batch_end = (i+1) * batch_size if (i+1) * batch_size < len(doc) else len(doc)-1\n",
        "    batch = doc_sentences[batch_start: batch_end]\n",
        "    if batch:\n",
        "      preds = predict(model, batch, doc) \n",
        "      scores = scores + preds.tolist() \n",
        "\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kurCk1thsgdS"
      },
      "outputs": [],
      "source": [
        "def calc_sentence_score(batched_df, model):\n",
        "  '''\n",
        "  This is the function that actually claculates the predicted score for each sentence\n",
        "\n",
        "  It results in a new column in the dataframe with the predcited score for each sentence\n",
        "  '''\n",
        "\n",
        "  all_pred_scores = []\n",
        "  case_text = batched_df['case_text']\n",
        "  summaries = batched_df['summaries']\n",
        "  batches = batched_df['512_batches']\n",
        "  batch_sentences = batched_df['batch_sentences']\n",
        "\n",
        "  #Iterating though each case\n",
        "  for i in tqdm(range(len(case_text))):\n",
        "    if i % 10 == 0 and i != 0:\n",
        "      print('Done with', i, 'cases, New File Saved')\n",
        "      save_as_pickle('/content/gdrive/MyDrive/Thesis/Data/all_pred_scores.pickle', all_pred_scores)\n",
        "\n",
        "\n",
        "    case_pred_outcomes = []\n",
        "    relevant_batches = batches[i]\n",
        "    relevant_batch_sentences = batch_sentences[i]\n",
        "\n",
        "    #Iterating through each batch in the case\n",
        "    for j in range(len(relevant_batches)):\n",
        "\n",
        "      key_batch = relevant_batches[j]\n",
        "      key_sents = relevant_batch_sentences[j]\n",
        "\n",
        "      #Generating list of sents from each batch, list of predicted outcomes, list of whether each sentence is in the summmary, and whether each batch contains a summary\n",
        "      scores = batch_and_score(key_batch, model, key_sents)\n",
        "\n",
        "      case_pred_outcomes.append(scores) #Should be the scores for each batch in a list\n",
        "  \n",
        "    all_pred_scores.append(case_pred_outcomes)\n",
        "\n",
        "  batched_df['pred_outcomes'] = all_pred_scores\n",
        "  \n",
        "  return batched_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tI73lf7zcYF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "scored_df = calc_sentence_score(smaller_df, extractive_model)\n",
        "\n",
        "filename = '/content/gdrive/MyDrive/Thesis/Data/scored_df.pickle'\n",
        "save_as_pickle(filename, scored_df)\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "print('Aggregation Preprocessing took:', duration)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_scores = pd.read_pickle('/content/gdrive/MyDrive/Thesis/Data/all_pred_scores.pickle')"
      ],
      "metadata": {
        "id": "tP9i971VPusg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scored_df)"
      ],
      "metadata": {
        "id": "JsNlAioUQVQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Calculate the positional weight for each batch and create a new column that mulitplies every score by the positional weight of its batch"
      ],
      "metadata": {
        "id": "F2XNshQdeI4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "scored_df = pd.read_pickle('/content/gdrive/MyDrive/Thesis/Data/scored_df.pickle')"
      ],
      "metadata": {
        "id": "O05YA3XCvMOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc5kPMdhADOa"
      },
      "outputs": [],
      "source": [
        "def custom_gauss_score(pos, a = 0.03065278, b = 0.40486706, c = 0.29048349): #Got these variables from the data exploration code\n",
        "  '''\n",
        "  Write a function calculates the gauss weight based on position of the batch\n",
        "  '''\n",
        "  return a * np.exp(-(pos - b)**2 / (2 * c**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mRMwOKsz_k6"
      },
      "outputs": [],
      "source": [
        "def create_pos(inference_df):\n",
        "  '''\n",
        "  This function looks at each case and multiplies the outcomes in each batch by the gaussian result from the position of that batch within the case\n",
        "  Need to nomralize the number of batches on a scale from 0 to 1 and calcualte the gauss\n",
        "  '''\n",
        "  all_pos_preds = []\n",
        "\n",
        "  for index, row in inference_df.iterrows():\n",
        "\n",
        "    batches = row['512_batches']\n",
        "    pred_for_sentences = row['pred_outcomes']\n",
        "\n",
        "    num_batches = len(batches)\n",
        "\n",
        "    positions = np.linspace(0, 1, num_batches) #Calculating position of each batch based on number of batches\n",
        "\n",
        "    updated_pos_preds = []\n",
        "    for i in range(num_batches):\n",
        "\n",
        "      p = positions[i] #The position of the batch\n",
        "      preds_for_batch_sents = pred_for_sentences[i] #list of sentence scores within a batch\n",
        "      gauss_score = custom_gauss_score(p)\n",
        "\n",
        "      updated_values = [elem[0] * gauss_score for elem in preds_for_batch_sents] #Multiplying each element in a batch by its weight\n",
        "\n",
        "      updated_pos_preds.append(updated_values)\n",
        "\n",
        "    all_pos_preds.append(updated_pos_preds)\n",
        "\n",
        "  inference_df['pos_update'] = all_pos_preds\n",
        "\n",
        "  return inference_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSdNM3TPewX7"
      },
      "outputs": [],
      "source": [
        "scored_df = create_pos(scored_df)\n",
        "save_as_pickle('/content/gdrive/MyDrive/Thesis/Data/inference_df.pickle', scored_df)\n",
        "\n",
        "print(scored_df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scored_df['pos_update'])"
      ],
      "metadata": {
        "id": "coKAti_1xQ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa_iupd9DZYU"
      },
      "source": [
        "### 3. Calculate the positional weight for each batch and create a new column that mulitplies every score by the positional weight of its batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgm8T6HrDdpU"
      },
      "outputs": [],
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "class BatchBertClass(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", in_features=768):\n",
        "        super(BatchBertClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(model_name)\n",
        "        self.pre_classifier = torch.nn.Linear(in_features, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 1)\n",
        "        self.classifierSigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, doc_ids, doc_mask):\n",
        "\n",
        "        doc_output = self.l1(input_ids=doc_ids, attention_mask=doc_mask) \n",
        "        doc_embeddings = mean_pooling(doc_output, doc_mask)\n",
        "        \n",
        "        pooler = self.pre_classifier(doc_embeddings) \n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        output = self.classifierSigmoid(output) \n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIvzsMmESe3O"
      },
      "outputs": [],
      "source": [
        "batches_state_dict = torch.load(\"/content/gdrive/MyDrive/Thesis/Models/batch_label_state_dict.pt\")\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "batch_class_model = BatchBertClass()\n",
        "batch_class_model.load_state_dict(batches_state_dict)\n",
        "batch_class_model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qW1AvC-S3uW"
      },
      "outputs": [],
      "source": [
        "def predict_batch(model, doc):\n",
        "  '''\n",
        "  This function makes the actual predictions\n",
        "  '''\n",
        " \n",
        "  doc_id, doc_mask = get_tokens([doc],tokenizer)\n",
        "  doc_id, doc_mask = torch.tensor(doc_id, dtype=torch.long),torch.tensor(doc_mask, dtype=torch.long)\n",
        "\n",
        "  pred = model(doc_id, doc_mask)\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "YXNo5_W1rbRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgYvemcDUhLO"
      },
      "outputs": [],
      "source": [
        "def create_bin_class(inference_df):\n",
        "  '''\n",
        "  This function looks at each case and multiplies the outcomes in each batch by the gaussian result from the position of that batch within the case\n",
        "  Need to nomralize the number of batches on a scale from 0 to 1 and calcualte the gauss\n",
        "  '''\n",
        "  all_pos_preds = []\n",
        "\n",
        "  for index, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
        "\n",
        "    batches = row['512_batches']\n",
        "    pred_for_sentences = row['pred_outcomes']\n",
        "\n",
        "    updated_pos_preds = []\n",
        "    for i in range(len(batches)):\n",
        "\n",
        "      batch = batches[i]\n",
        "\n",
        "      preds_for_batch_sents = pred_for_sentences[i] #list of sentence scores within a batch\n",
        "      batch_score =  predict_batch(batch_class_model, batch) #Fill in here with\n",
        "      batch_score = batch_score.tolist()[0][0]\n",
        "\n",
        "      updated_values = [elem[0] * batch_score for elem in preds_for_batch_sents] #Multiplying each element in a batch by its weight\n",
        "\n",
        "      updated_pos_preds.append(updated_values)\n",
        "\n",
        "    all_pos_preds.append(updated_pos_preds)\n",
        "\n",
        "  inference_df['bin_class_update'] = all_pos_preds\n",
        "\n",
        "  return inference_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNpRJw2TeeIN"
      },
      "outputs": [],
      "source": [
        "scored_df = create_bin_class(scored_df)\n",
        "print(scored_df.columns)\n",
        "save_as_pickle('/content/gdrive/MyDrive/Thesis/Data/scored_df.pickle', scored_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scored_df['bin_class_update'])"
      ],
      "metadata": {
        "id": "hwQQCAzWx3_Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}